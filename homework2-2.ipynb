{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用データ  \n",
    "1つ目の課題と同様に、doc_dir内のdoc1とdoc2を使用。  \n",
    "それぞれ50件ずつ抽出して、100件の文書データを使用  \n",
    "すなわち、文書番号0から49はdoc1、文書番号50から99はdoc2である。  \n",
    "・ doc1 : 夏目漱石「私の個人主義」  \n",
    "・ doc2 : 宮沢賢治「セロ弾きのゴーシュ」  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "doc_origin_list = []\n",
    "for i in range(1,3):\n",
    "    with open(\"./doc_dir/doc{}.txt\".format(i)) as f:\n",
    "        s = f.read()\n",
    "        doc_origin_list.append(s.split(\"\\n\"))\n",
    "\n",
    "# １００件のデータを取得\n",
    "raw_docs = []\n",
    "for i in range(2):\n",
    "    for j in range(100):\n",
    "        if doc_origin_list[i][j] != \"\":\n",
    "            raw_docs.append(doc_origin_list[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning_text(text):\n",
    "    # 改行コードの削除\n",
    "    text = re.sub(r\"[\\r\\n]\", '', text)    \n",
    "    # 空欄の削除\n",
    "    text = re.sub(r\"[\\u3000 \\t]\", '', text)    \n",
    "    # 句読点などの記号を削除\n",
    "    text = re.sub(\"[。、―「」!?｜［］＃#)@:]\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "for doc in raw_docs:\n",
    "    new_doc = cleaning_text(doc)\n",
    "    doc_list.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome import tokenfilter, charfilter\n",
    "\n",
    "char_filters = [charfilter.UnicodeNormalizeCharFilter()]\n",
    "token_filters = [tokenfilter.POSKeepFilter(['名詞'])]\n",
    "tokenizer = Tokenizer()\n",
    "analyzer = Analyzer(char_filters, tokenizer, token_filters)\n",
    "\n",
    "def remove_one_length(token_list):\n",
    "    for word in token_list:\n",
    "        if len(word) == 1:\n",
    "            token_list.remove(word)\n",
    "\n",
    "doc_token = []\n",
    "for doc in doc_list:\n",
    "    tmp_token = []\n",
    "    for token in analyzer.analyze(doc):\n",
    "        tmp_token.append(token.surface)\n",
    "        remove_one_length(tmp_token)\n",
    "    doc_token.append(tmp_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['時間', '批評', 'はず', 'ため', '懊悩', '参考', '注文', '本領', 'ネルソン', 'さん', '奔走']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_token[0][:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1219 unique tokens: ['あと', 'あなた', 'いっぱい', 'うち', 'お話']...)\n",
      "corpus(前5つ) : [(1, 1), (3, 1), (7, 1), (12, 1), (16, 2)]\n"
     ]
    }
   ],
   "source": [
    "# 辞書 : 単語ID・単語・単語出現回数\n",
    "dic = Dictionary(doc_token)\n",
    "print(dic)\n",
    "# コーパス・・・(単語ID,出現頻度)\n",
    "corpus = [dic.doc2bow(s) for s in doc_token]\n",
    "print(\"corpus(前5つ) : \"+str(corpus[1][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数のトピック数を用いて結果を出力\n",
    "k= 2,5,10,15で実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakabayashi/Desktop/M1 前期 授業/情報システム論実習・授業/0630/text_analysis_homework/env/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "lda = LdaModel(corpus = corpus, id2word = dic, num_topics = k, alpha = \"auto\")\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dic, n_jobs = 1, sort_topics = False)\n",
    "pyLDAvis.save_html(lda_display,'vis{}.html'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakabayashi/Desktop/M1 前期 授業/情報システム論実習・授業/0630/text_analysis_homework/env/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "lda = LdaModel(corpus = corpus, id2word = dic, num_topics = k, alpha = \"auto\")\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dic, n_jobs = 1, sort_topics = False)\n",
    "pyLDAvis.save_html(lda_display,'vis{}.html'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakabayashi/Desktop/M1 前期 授業/情報システム論実習・授業/0630/text_analysis_homework/env/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "lda = LdaModel(corpus = corpus, id2word = dic, num_topics = k, alpha = \"auto\")\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dic, n_jobs = 1, sort_topics = False)\n",
    "pyLDAvis.save_html(lda_display,'vis{}.html'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakabayashi/Desktop/M1 前期 授業/情報システム論実習・授業/0630/text_analysis_homework/env/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "k=15\n",
    "lda = LdaModel(corpus = corpus, id2word = dic, num_topics = k, alpha = \"auto\")\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dic, n_jobs = 1, sort_topics = False)\n",
    "pyLDAvis.save_html(lda_display,'vis{}.html'.format(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
